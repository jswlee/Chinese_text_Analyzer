{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4c547a-09c0-4239-b3c2-b542ad76c918",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "### Jonathan Lee - 李小慧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dbc92561-a9a5-4e45-b69c-c53931a4469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import logging\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4de70c-3806-47c4-9f47-2afb5331b102",
   "metadata": {},
   "source": [
    "## Motivations and Prelimary Findings\n",
    "When asked to give my thoughts on characteristics of both Modern and Old Chinese with respect to word order and distribution, it occurred to me that it would be an interesting experiment to see what examples I could find of these phenomena in various Old Chinese texts, especially given the fact that it is difficult to search for this in a traditional linguistic corpus. As we were given an example of how Old Chinese does inflect for case as shown in the Analects in interrogative and negation sentences (e.g., 吾誰欺？ and 不我與, respectively), I decided to first look at the Analects and Zuozhuan (similar time period). By using the Natural Language Processing (NLP) toolkit known as Stanza, I was able to identify several similar cases by scanning the text for the same POS patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a939b5-6482-4d92-9774-5aadcdceef82",
   "metadata": {},
   "source": [
    "#### Stanza\n",
    "I decided to use a collection of NLP tools in the form of Stanza (https://stanfordnlp.github.io/stanza/index.html). According to their website, Staza is \"a collection of accurate and efficient tools for the linguistic analysis of many human languages.\" I chose this because they have tools to work specifically with Literary Chinese. In particular, it can tokenize (segment text strings into words) and attach parts-of-speech (POS) to each token in a Literary Chinese text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3fba853a-325e-4c36-8388-cd154a79a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the logging level to ERROR to suppress informational messages\n",
    "logging.getLogger('stanza').setLevel(logging.ERROR)\n",
    "\n",
    "#Download the language model for Literary Chinese\n",
    "stanza.download('lzh')\n",
    "nlp = stanza.Pipeline(lang='lzh', processor='tokenize.pos', logging_level='ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab86cefb-014f-40a5-951a-94c227aaba07",
   "metadata": {},
   "source": [
    "### File Processing\n",
    "* First, I started by finding a version of the Analects and Zuozhuan online and copying them to files with UTF-8 encoding (in order to work with Chinese texts). Below, I read the files from my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bc5d6bff-0132-4f72-b1f5-7c51202d6aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of Analects with Punctuation\n",
      "學而第一\n",
      "1.子曰：\n",
      "「學而時習之，不亦說\n",
      "乎？有朋自遠方來，不\n",
      "亦樂乎？人不知而不慍\n",
      "，不亦君子乎？」（1\n",
      ".1）\n",
      "2.有子曰：\n",
      "「其為人也孝弟，而好\n",
      "犯上者，鮮矣；不好犯\n",
      "上，而好作亂者，未之\n",
      "有也。君子務本，本立\n"
     ]
    }
   ],
   "source": [
    "#Read file\n",
    "file_path_1 = \"/Users/jlee/Desktop/UH Spring 2024 Courses/CHN 631C/CHN 631C Project/Analects.txt\"\n",
    "with open(file_path_1, 'r', encoding='utf-8') as file:\n",
    "    analects = file.read()\n",
    "\n",
    "#Preview of the text\n",
    "print(\"Preview of Analects with Punctuation\")\n",
    "for i in range(0,100,10):\n",
    "    print(analects[i:i+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "407ca949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of Zuozhuan with Punctuation\n",
      "\n",
      "\n",
      "春秋左传\n",
      "左丘明\n",
      " 著\n",
      "\n",
      "\n",
      "\n",
      "目录\n",
      "\n",
      "\n",
      "隐公（元年～十一年）\n",
      "⋯⋯⋯⋯⋯⋯⋯⋯⋯⋯\n",
      "⋯⋯⋯⋯⋯001桓公\n",
      "（元年～十八年）⋯⋯\n",
      "⋯⋯⋯⋯⋯⋯⋯⋯⋯⋯\n",
      "⋯⋯⋯019庄公（元\n",
      "年～三十二年）⋯⋯⋯\n",
      "⋯⋯⋯⋯⋯⋯⋯⋯⋯⋯\n"
     ]
    }
   ],
   "source": [
    "file_path_2 = \"/Users/jlee/Desktop/UH Spring 2024 Courses/CHN 631C/CHN 631C Project/春秋左传.txt\"\n",
    "with open(file_path_2, 'r', encoding='utf-8') as file:\n",
    "    zuozhuan = file.read()\n",
    "\n",
    "#Preview of the text\n",
    "print(\"Preview of Zuozhuan with Punctuation\")\n",
    "for i in range(0,100,10):\n",
    "    print(zuozhuan[i:i+10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26898216-463e-4632-a575-fb8d841bd279",
   "metadata": {},
   "source": [
    "* Next, I removed everything but the characters to get a simpler and more straightforward text to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c2ce7072-1712-49f5-8afe-65bf5df6b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean up Chinese text with  utf-8 encoding\n",
    "def clean_text(text):\n",
    "    #Remove special punctuation with regex\n",
    "    cleaned_text = re.sub(r\"[。「」、﹑.()（）？！：，；『』⋯～“”]\",'', text)\n",
    "    #Remove numbers with regex\n",
    "    cleaned_text = re.sub(r'\\d+','', cleaned_text)\n",
    "    #Remove spaces\n",
    "    cleaned_text = re.sub(r' ','', cleaned_text)\n",
    "    cleaned_text = re.sub(r'  ','', cleaned_text)\n",
    "    cleaned_text = re.sub(r' ','', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3ce16d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of Analects without Punctuation\n",
      "學而第一\n",
      "子曰學而時\n",
      "習之不亦說乎有朋自遠\n",
      "方來不亦樂乎人不知而\n",
      "不慍不亦君子乎\n",
      "有子\n",
      "曰其為人也孝弟而好犯\n",
      "上者鮮矣不好犯上而好\n",
      "作亂者未之有也君子務\n",
      "本本立而道生孝弟也者\n",
      "其為仁之本與\n",
      "子曰巧\n",
      "言令色鮮矣仁\n",
      "曾子曰\n"
     ]
    }
   ],
   "source": [
    "cleaned_analects = clean_text(analects)\n",
    "#A look at the first 100 characters\n",
    "print(\"Preview of Analects without Punctuation\")\n",
    "for i in range(0,100,10):\n",
    "    print(cleaned_analects[i:i+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bd1c93f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of Zuozhuan without Punctuation\n",
      "\n",
      "\n",
      "春秋左传\n",
      "左丘明\n",
      "著\n",
      "\n",
      "\n",
      "\n",
      "目录\n",
      "\n",
      "隐\n",
      "公元年十一年桓公元年\n",
      "十八年庄公元年三十二\n",
      "年闵公元年二年僖公元\n",
      "年三十三年文公元年十\n",
      "八年宣公元年十八年成\n",
      "公元年十八年襄公元年\n",
      "三十一年昭公元年三十\n",
      "二年定公元年十五年哀\n"
     ]
    }
   ],
   "source": [
    "cleaned_zuozhuan = clean_text(zuozhuan)\n",
    "#A look at the first 100 characters\n",
    "print(\"Preview of Zuozhuan without Punctuation\")\n",
    "for i in range(0,100,10):\n",
    "    print(cleaned_zuozhuan[i:i+10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "603e8058-7f32-4fcc-9bbf-4374addf286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as /Users/jlee/Desktop/UH Spring 2024 Courses/CHN 631C/CHN 631C Project/Analects_No_Punct\n",
      "File saved as /Users/jlee/Desktop/UH Spring 2024 Courses/CHN 631C/CHN 631C Project/Zuozhuan_No_Punct\n"
     ]
    }
   ],
   "source": [
    "#Save the cleaned text to a file on my computer\n",
    "cleaned_analects_name = \"/Users/jlee/Desktop/UH Spring 2024 Courses/CHN 631C/CHN 631C Project/Analects_No_Punct\"\n",
    "with open(cleaned_analects_name, 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_analects)\n",
    "print(f\"File saved as {cleaned_analects_name}\")\n",
    "\n",
    "cleaned_zuozhuan_name = \"/Users/jlee/Desktop/UH Spring 2024 Courses/CHN 631C/CHN 631C Project/Zuozhuan_No_Punct\"\n",
    "with open(cleaned_zuozhuan_name, 'w', encoding='utf-8') as file:\n",
    "    file.write(cleaned_analects)\n",
    "print(f\"File saved as {cleaned_zuozhuan_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2cacdb-2f7c-4021-87a2-0de0b3f9072b",
   "metadata": {},
   "source": [
    "### Tokenization and POS tagging\n",
    "* The next main task was to tokenize and attach POS tags to the cleaned text as was done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e4a370e5-ce2a-4565-aba8-4845b0819282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fuction to tokenize and attach POS tags\n",
    "def pos_and_tokenize(cleaned_text, output_filename):\n",
    "    doc = nlp(cleaned_text)\n",
    "    tokens = []\n",
    "    pos_tags = []\n",
    "    #Tokenization and POS tagging\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            tokens.append(word.text)\n",
    "            pos_tags.append(word.pos)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'TOKEN': tokens,\n",
    "        'POS': pos_tags\n",
    "    })\n",
    "    df.to_csv(f'{output_filename}_pos_tok.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c875a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tok_analects = pos_and_tokenize(cleaned_analects, \"cleaned_analects\")\n",
    "pos_tok_zuozhuan = pos_and_tokenize(cleaned_zuozhuan, \"cleaned_zuozhuan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b5b729-24a2-4036-afe3-abcb8405f0a6",
   "metadata": {},
   "source": [
    "* Below, we can see the results of Stanza's tokenization and POS. At least for Literary Chinese, Stanza tends to simply split each word into a single character. However, there are exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "716ef179-8bdf-48ea-8618-211f1a49519c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>學</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>而</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>第</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>一</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>子</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>曰</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>學</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>而</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>時</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>習</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TOKEN    POS\n",
       "0     學   VERB\n",
       "1     而  CCONJ\n",
       "2     第   NOUN\n",
       "3     一    NUM\n",
       "4     子   NOUN\n",
       "5     曰   VERB\n",
       "6     學   VERB\n",
       "7     而  CCONJ\n",
       "8     時   NOUN\n",
       "9     習   VERB"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tok_analects.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6f3051c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>春</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>秋</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>左</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>传</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>左</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>丘</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>明</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>著</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>目</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>录</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TOKEN   POS\n",
       "0     春  NOUN\n",
       "1     秋  NOUN\n",
       "2     左  VERB\n",
       "3     传  NOUN\n",
       "4     左  NOUN\n",
       "5     丘  NOUN\n",
       "6     明  VERB\n",
       "7     著  VERB\n",
       "8     目  NOUN\n",
       "9     录  NOUN"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tok_zuozhuan.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd41c40-b22a-4453-b674-46aa83af6e6f",
   "metadata": {},
   "source": [
    "* In the code snippets below, we see examples of cases where Stanza identified multisyllabic words. In general, disyllabic words are names or specific nouns like 君子 that would have no meaning if the characters were separated. In the Analects, Stanza was able to identify 601 cases of disyllabic words and 5 cases of larger multisyllabic words (which were all just numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2cf70576-1ff8-4f12-9af0-85a6f26f6e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>君子</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>有子</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>君子</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>曾子</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>弟子</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TOKEN    POS\n",
       "33     君子   NOUN\n",
       "35     有子  PROPN\n",
       "63     君子   NOUN\n",
       "90     曾子  PROPN\n",
       "140    弟子   NOUN"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disyllabic_words_analects = pos_tok_analects[pos_tok_analects['TOKEN'].apply(lambda word: len(word) == 2)]\n",
    "disyllabic_words_analects.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a406f184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>隐公</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>十一</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>桓公</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>十八</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>庄公</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TOKEN   POS\n",
       "10    隐公  NOUN\n",
       "13    十一   NUM\n",
       "15    桓公  NOUN\n",
       "18    十八   NUM\n",
       "20    庄公  NOUN"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disyllabic_words_zuozhuan = pos_tok_zuozhuan[pos_tok_zuozhuan['TOKEN'].apply(lambda word: len(word) == 2)]\n",
    "disyllabic_words_zuozhuan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "046fac73-9b49-41e9-945d-3324a3971807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 601 disyllabic words in the Analects and 6172 in Zuozhuan\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {disyllabic_words_analects.shape[0]} disyllabic words in the Analects and {disyllabic_words_zuozhuan.shape[0]} in Zuozhuan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "740b890f-983f-43c2-b758-1c373bddc291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>三百一</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5880</th>\n",
       "      <td>四十五十</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>六七十</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7529</th>\n",
       "      <td>五六十</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>六七十</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TOKEN  POS\n",
       "503    三百一  NUM\n",
       "5880  四十五十  NUM\n",
       "7527   六七十  NUM\n",
       "7529   五六十  NUM\n",
       "7700   六七十  NUM"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_multi_syllabic_words_analects = pos_tok_analects[pos_tok_analects['TOKEN'].apply(lambda word: len(word) > 2)].head()\n",
    "other_multi_syllabic_words_analects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "690259e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOKEN</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>三十二</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>三十三</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>三十一</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>三十二</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>二十七</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TOKEN  POS\n",
       "23   三十二  NUM\n",
       "35   三十三  NUM\n",
       "55   三十一  NUM\n",
       "61   三十二  NUM\n",
       "73   二十七  NUM"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_multi_syllabic_words_zuozhuan = pos_tok_zuozhuan[pos_tok_zuozhuan['TOKEN'].apply(lambda word: len(word) > 2)].head()\n",
    "other_multi_syllabic_words_zuozhuan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fc3ff4d0-3e44-4e39-ac77-d2e469d7c16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 disyllabic words in the Analects and 5 in Zuozhuan\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {other_multi_syllabic_words_analects.shape[0]} disyllabic words in the Analects and {other_multi_syllabic_words_zuozhuan.shape[0]} in Zuozhuan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4d31c-c155-4427-b2a4-5ad9cc309c61",
   "metadata": {},
   "source": [
    "* Next up is to check the POS that Stanza used for each word in 吾誰欺 and for 不 to determine what pattern to search for in the text. The result was it identified 吾 as a pronoun, 誰 a pronoun, and 欺 a verb. Stanza also tagged 不 as an adverb, so that would indicate for cases like 不我與, I would need to look for adverb-pronoun-verb occurences. While this might be too general for some cases, I decided to use it for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1791583b-9108-4885-b0d2-6e3b4da0e0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     TOKEN   POS\n",
      "5631     吾  PRON\n",
      "5632     誰  PRON\n",
      "5633     欺  VERB\n"
     ]
    }
   ],
   "source": [
    "#Function for finding a sequence of tokens in the dataframe. It's essentially just a ctrl-f.\n",
    "\n",
    "def get_sequence_df(df, sequence):\n",
    "    for i in range(len(df) - len(sequence) + 1):\n",
    "        if all(df.iloc[i + j]['TOKEN'] == sequence[j] for j in range(len(sequence))):\n",
    "            return df.iloc[i:i+len(sequence)]\n",
    "    return pd.DataFrame()  # Return an empty DataFrame if sequence is not found\n",
    "\n",
    "sequence = ['吾', '誰', '欺']\n",
    "sequence_df = get_sequence_df(pos_tok_analects, sequence)\n",
    "\n",
    "print(sequence_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6194d2a-5485-449b-88ce-592c58ea4255",
   "metadata": {},
   "source": [
    "* Armed with the knowledge that I am looking for PRON-PRON-VERB and ADV-PRON-VERB sequences, I create a function to find those patterns and then return the reults in a list of strings, which ended up finding 21 cases for the first pattern and 33 for the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "876f79f7-66a6-430c-8620-429720d68708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['斯之謂', '其何以', '其或繼', '吾何以', '之何如', '是吾憂', '予何大', '吾誰欺', '何其聞', '爾何如', '爾何如', '爾何如', '何其徹', '之或問', '之何如', '諸己小', '斯某在', '斯之謂', '何其廢', '何其拒', '是之甚']\n",
      "['未之有', '曾是以', '奚其為', '未之見', '莫己知', '猶吾大', '猶吾大', '莫吾猶', '未之有', '則之蕩', '不其然', '何其多', '未之思', '毋吾以', '不吾知', '則何以', '毋自辱', '奚其正', '亦奚以', '不吾以', '莫予違', '莫之違', '莫之違', '豈其然', '莫之知', '不己知', '莫我知', '莫己知', '未之學', '不己知', '不我與', '又誰怨', '猶之與']\n",
      "['为之请', '是之谓', '之虽及', '为其少', '之彼则', '是其生', '此其以', '之是行', '我吾求', '此其在', '此其昌', '子其死', '子其行', '之何稽', '吾其定', '之何对', '之我怠', '其何以', '吾其奔', '其何以', '之是求', '之何庸', '之其若', '我吾使', '子是寡', '吾何以', '之吾舍', '之我辞', '吾自惧', '我何以', '之彼骄', '之吾以', '吾其死', '无自立', '此之谓', '之何杀', '之其先', '之何以', '余是以', '此之谓', '之是齐', '何其以', '其自为', '是之谓', '之之明', '之何曰', '之是弃', '之是以', '其何以', '其孰以', '将何以', '是我有', '我是以', '我是以', '我是以', '其何如', '之其御', '之我毙', '之其可', '之是以', '是之谓', '我是欲', '余余恐', '之何对', '之何’', '之或推', '为自逸', '之其若', '将何以', '其或难', '之是以', '何其以', '之何以', '是之谓', '此之谓', '是之谓', '此之谓', '之是以', '我我克', '我是以', '其何以', '之语问', '我其收', '我其拱', '之何以', '子余祭', '为之歌', '为之歌', '此之谓', '为之歌', '此则明', '为之歌', '为之歌', '为之歌', '之见舞', '之孰杀', '吾其与', '之我有', '何是以', '其何以', '之何如', '之我闻', '子何以', '是是以', '之是委', '是之谓', '将何以', '此之谓', '吾何以', '吾是以', '子其无', '其何如', '之其爱', '是之谓', '是之谓', '子其将', '之其藏', '之其出', '之其藏', '我何及', '之其蔑', '其何事', '之是得', '己余将', '是之谓', '之或告', '之其若', '之我在', '之是以', '之或谮', '之其与', '将何以', '之何对', '之吾助', '其何以', '之是以', '是其征', '之何对', '我吾告', '己是以', '之何以', '之子奉', '是是杀', '此之谓', '我孰利', '之其造', '女何以', '吾是以', '之,在', '之何以', '此之谓', '我我伐', '何其使', '之是反', '是余将', '之何以', '之子言', '之是勤', '此之谓', '是我迋', '之或告', '之我御', '见我在', '为何如', '其何以', '之何对', '子我曰', '是其在', '之子行', '是我寡', '其何以', '将何为', '我吾与', '吾是以', '之是以', '之是无', '吾其入', '之自出']\n",
      "['故为蔑', '将自及', '将自焚', '不自送', '苟自救', '善自为', '将自用', '始吾敬', '不自作', '亦自杀', '遂自刖', '以自纳', '莫之与', '故是以', '将自毙', '固,安', '必是时', '唯我知', '凡我同', '将何如', '不其然', '且吾闻', '何其速', '未之服', '实自败', '将无及', '不自讨', '不我知', '为之降', '以自取', '故书以', '若我出', '遂自阴', '莫之继', '遂自亡', '即自逆', '未之闻', '则无及', '为是犯', '凡自虐', '将为戮', '非他寡', '且是以', '为吾望', '斯是用', '不我纳', '将何以', '又何如', '与吾同', '以自为', '惟其有', '必自及', '不我救', '以自封', '不吾疾', '凡我同', '凡我同', '毋是翦', '未是有', '唯尔有', '与吾同', '常之有', '遂自为', '为己死', '为己亡', '非其私', '若无侵', '且子展', '不其然', '将或弭', '则我失', '莫之与', '且吾因', '不余辟', '莫之止', '只见疏', '且无使', '常之有', '皆自朝', '不我顺', '遂自止', '非我有', '未之见', '何其释', '不吾叛', '又何以', '莫之知', '未之闻', '又何以', '既自见', '若吾以', '始吾有', '则自取', '未之祀', '为之归', '则是效', '且其繇', '共自文', '亦自饮', '殆其失', '以自成', '未之治', '以为大', '不我与', '早自图', '皆自杀', '未之致', '不余畀', '必自取', '故之以', '莫之如', '莫之亢', '且吾以', '不我觌', '莫之奔', '莫之报', '不吾远', '因之有', '莫之死', '故是以', '未之有', '果自言', '不余欺', '常之有', '不吾废', '非我生', '遂自杀', '以自利', '以自安', '以自危', '即其安', '将自有', '凡我同', '且己无', '皆自坏', '莫之治', '且吾尤', '唯我事', '然无以', '然我往', '未之有', '遂自刭', '未之有', '未之有', '未之闻', '未之改', '与子有', '苟我寡', '然吾受', '何自厉', '非我因', '未女恤']\n"
     ]
    }
   ],
   "source": [
    "# Example: Find sequences of PRON-PRON-VERB\n",
    "pattern1 = ['PRON','PRON','VERB']\n",
    "pattern2 = ['ADV','PRON','VERB']\n",
    "def find_pattern_in_df(df, pattern):\n",
    "    results = []\n",
    "    pattern_str = ''.join(pattern)\n",
    "    for i in range(len(df) - len(pattern) + 1):\n",
    "        if df.iloc[i:i+len(pattern)]['POS'].tolist() == pattern:\n",
    "            tokens = df.iloc[i:i+len(pattern)]['TOKEN'].tolist()\n",
    "            results.append(''.join(tokens))\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "pattern_str1_analects = find_pattern_in_df(pos_tok_analects, pattern1)\n",
    "print(pattern_str1_analects)\n",
    "\n",
    "pattern_str2_analects = find_pattern_in_df(pos_tok_analects, pattern2)\n",
    "print(pattern_str2_analects)\n",
    "\n",
    "pattern_str1_zuozhuan = find_pattern_in_df(pos_tok_zuozhuan, pattern1)\n",
    "print(pattern_str1_zuozhuan)\n",
    "\n",
    "pattern_str2_zuozhuan = find_pattern_in_df(pos_tok_zuozhuan, pattern2)\n",
    "print(pattern_str2_zuozhuan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "295ca0ca-a1b0-4141-a9da-e344a566ca85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21 instances of PRON-PRON-VERB in the text.\n",
      "There are 33 instances of ADV-PRON-VERB in the text.\n",
      "There are 178 instances of PRON-PRON-VERB in the text.\n",
      "There are 152 instances of ADV-PRON-VERB in the text.\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(pattern_str1_analects)} instances of PRON-PRON-VERB in the Analects.')\n",
    "print(f'There are {len(pattern_str2_analects)} instances of ADV-PRON-VERB in the Analects.')\n",
    "print(f'There are {len(pattern_str1_zuozhuan)} instances of PRON-PRON-VERB in the Zuozhuan.')\n",
    "print(f'There are {len(pattern_str2_zuozhuan)} instances of ADV-PRON-VERB in the Zuozhuan.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb054a34-623f-464e-aaac-5aa725dfd651",
   "metadata": {},
   "source": [
    "* The last task (for now) was to actually return the sentences where these sequences occur. I start by defining a function to split the text into sentences and highlight the example 3-character sequences with underscores for readability. I added in functionality to check whether the sentence contains a question, since the PRON-PRON-VERB pattern should only occur in an interrogative sentence, in theory. I also added functionality to check for specific characters at the front of the pattern, since negation sentences in the Analects and Zuozhuan are relatively limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "05bb9d6c-dd87-4dc9-8cc1-5168d1e7399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_highlight_sentences(text, search_strings, is_question=False, start_chars=None):\n",
    "    # Split the text into sentences based on periods (or appropriate punctuation)\n",
    "    sentences = text.split('。')\n",
    "    \n",
    "    # Compile a pattern to match and remove the trailing Chinese open parenthesis followed by a number\n",
    "    remove_pattern = re.compile(r'（\\d+$')\n",
    "    \n",
    "    # Apply the removal pattern to each sentence\n",
    "    sentences = [remove_pattern.sub('', sentence) for sentence in sentences]\n",
    "    \n",
    "    highlighted_sentences = []\n",
    "    \n",
    "    # Filter search_strings if start_chars is provided\n",
    "    if start_chars:\n",
    "        filtered_search_strings = [s for s in search_strings if s.startswith(start_chars)]\n",
    "    else:\n",
    "        filtered_search_strings = search_strings\n",
    "    \n",
    "    # Compile a regular expression pattern for efficient matching\n",
    "    pattern = re.compile('|'.join([re.escape(s) for s in filtered_search_strings]))\n",
    "    \n",
    "    # Function to add highlights only if not already highlighted\n",
    "    def highlight(match):\n",
    "        matched_text = match.group(0)\n",
    "        # Check if the matched text is already highlighted\n",
    "        if matched_text.startswith('___') and matched_text.endswith('___'):\n",
    "            return matched_text  # Return as is if already highlighted\n",
    "        else:\n",
    "            return f\"___{matched_text}___\"  # Highlight if not already highlighted\n",
    "    \n",
    "    # Iterate through each sentence\n",
    "    for sentence in sentences:\n",
    "        # Strip leading and trailing whitespace from the sentence\n",
    "        sentence = sentence.strip()\n",
    "        \n",
    "        # Replace all occurrences of search strings with highlighted version\n",
    "        highlighted_sentence = pattern.sub(highlight, sentence)\n",
    "        \n",
    "        # Add condition based on 'question' parameter to filter sentences with Chinese question marks\n",
    "        if is_question:\n",
    "            # Only add the sentence if it contains a Chinese question mark\n",
    "            if '？' in highlighted_sentence and highlighted_sentence != sentence:\n",
    "                highlighted_sentences.append(highlighted_sentence)\n",
    "        else:\n",
    "            # Add the sentence if any replacements were made\n",
    "            if highlighted_sentence != sentence:\n",
    "                highlighted_sentences.append(highlighted_sentence)\n",
    "    \n",
    "    return highlighted_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf0325-01ff-4689-bdef-5a3e23f141a8",
   "metadata": {},
   "source": [
    "* The code below returns the pronoun-pronoun-verb example sentences. Note that certain instances such as 予何大 are not returned with a sentence for context. That is due to the fact that 予何大 is part of two sentences, but the pattern was identified as the punctuation was removed:\n",
    "\n",
    "5.  子畏於匡。曰：「文王既沒，文不在茲乎。天之章喪斯文也。後死者不得與於斯文也。天之未喪斯文也。匡人其如**予何**。」（9.5）\n",
    "6.  **大**宰問於子貢曰：「夫子聖者與！何其多能也？」子貢曰：「固天縱之將聖，又多能也。」子聞之曰：「大宰知我乎？吾少也賤，故多能鄙事。君子多乎哉？不多也！」（9.6）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "58fbb51f-5e47-45bb-8c6d-e54f04773a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences for PRON-PRON-VERB in the Analects\n",
      "\n",
      "_[1]_    」子貢曰：「詩云：『如切如磋，如琢如磨』，其___斯之謂___與？」子曰：「賜也，始可與言詩已矣，告諸往而知來者\n",
      "\n",
      "_[2]_    大車無輗，小車無軏，___其何以___行之哉？」（2.22）\n",
      "23.子張問：「十世可知也？」子曰：「殷因於夏禮，所損益，可知也；周因於殷禮，所損益，可知也\n",
      "\n",
      "_[3]_    」（3.25）\n",
      "26.子曰：「居上不寬，為禮不敬，臨喪不哀，___吾何以___觀之哉？」（3.26）\n",
      "里仁第四\n",
      "1.  子曰：「里仁為美\n",
      "\n",
      "_[4]_    」（7.1）\n",
      "2.  子曰：「默而識之，學而不厭，誨人不倦，何有於我哉？」（7.2）\n",
      "3.  子曰：「德之不修，學之不講，聞義不能徒，不善不能改，___是吾憂___也\n",
      "\n",
      "_[5]_    欲罷不能，既竭吾才，如有所立，卓爾；雖欲從之，末由也已！」（9.11）\n",
      "12.子疾病，子路使門人為臣，病聞，曰：「久矣哉，由之行詐也！無臣而為有臣，___吾誰欺___？欺天乎？且予與其死於臣之手也，無甯死於二三子之手乎！且予縱不得大葬，予死於道路乎？」（9.12）\n",
      "13.子貢曰：「有美玉於斯，韞(櫝)而藏諸？求善賈而沽諸？」子曰：「沽之哉！沽之哉！我待賈者也！」（9.13）\n",
      "14.子欲居九夷\n",
      "\n",
      "_[6]_    」子曰：「論篤是與，君子者乎？色莊者乎？」（11.19）\n",
      "20.子路問：「聞斯行諸？」子曰：「有父兄在，如之___何其聞___斯行之！」冉有問：「聞斯行諸？」子曰：「聞斯行之！」公西華曰：「由也問『聞斯行諸？』，子曰：『有父兄在』；求也問，『聞斯行諸？』子曰：『聞斯行之』\n",
      "\n",
      "_[7]_    「求，___爾何如___？」對曰：「方六七十，如五六十，求也為之，比及三年，可使足民；如其禮樂，以俟君子\n",
      "\n",
      "_[8]_    」「赤，___爾何如___？」對曰：「非曰能之，願學焉！宗廟之事，如會同，端章甫，願為小相焉\n",
      "\n",
      "_[9]_    」「點，___爾何如___？」鼓瑟希，鏗爾，舍瑟而作\n",
      "\n",
      "_[10]_    」（12.8）\n",
      "9.  哀公問於有若曰：「年饑，用不足，如之何？」有若對曰：「盍徹乎！」曰：「二，吾猶不足；如之___何其徹___也？」對曰：「百姓足，君孰不足？百姓不足，君孰與足？」（12.9）\n",
      "10.子張問「崇德，辨惑\n",
      "\n",
      "_[11]_    其___斯之謂___與？」（16.12）\n",
      "13.陳亢問於伯魚曰：「子亦有異聞乎？」對曰：「未也\n",
      "\n",
      "_[12]_    長幼之節，不可廢也；君臣之義，如之___何其廢___之？欲潔其身，而亂大倫\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highlighted_sentences = find_and_highlight_sentences(analects, pattern_str1_analects, is_question = True)\n",
    "print(\"Example sentences for PRON-PRON-VERB in the Analects\\n\")\n",
    "for i in range(len(highlighted_sentences)):\n",
    "    print(f'_[{i+1}]_    {highlighted_sentences[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3ff82-2fe8-45e5-9381-d270bc922e83",
   "metadata": {},
   "source": [
    "The code below returns the adverb-pronoun-verb example sentences. Note that certain instances such as are not returned with a sentence for context for the same reasons as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "54062a2b-21a3-4302-b245-6e81398fa2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences for ADV-PRON-VERB in the Analects\n",
      "\n",
      "_[1]_    學而第一\n",
      "1.子曰：「學而時習之，不亦說乎？有朋自遠方來，不亦樂乎？人不知而不慍，不亦君子乎？」（1.1）\n",
      "2.有子曰：「其為人也孝弟，而好犯上者，鮮矣；不好犯上，而好作亂者，___未之有___也\n",
      "\n",
      "_[2]_    蓋有之矣，我___未之見___也\n",
      "\n",
      "_[3]_    躬行君子，則吾___未之有___得\n",
      "\n",
      "_[4]_    」孔子曰：「才難，___不其然___乎，唐虞之際，於斯為盛，有婦人焉，九人而已\n",
      "\n",
      "_[5]_    」子曰：「___未之思___也，未何遠之有？」（9.30）\n",
      "鄉黨第十\n",
      "1.  孔子於鄉黨，恂恂如也，似不能言者\n",
      "\n",
      "_[6]_    居則曰：『___不吾知___也！』如或知爾，則何以哉？」子路率爾而對，曰：「千乘之國，攝乎大國之間閒，加之以師旅，因之以饑饉，由也為之，比及三年，可使有勇，且知方也\n",
      "\n",
      "_[7]_    」子曰：「其事也！如有政，雖___不吾以___，吾其與聞之！」（13.14）\n",
      "15.定公問：「一言而可以興邦，有諸？」孔子對曰：「言不可以若是其幾也！人之言曰：『為君難，為臣不易\n",
      "\n",
      "_[8]_    子曰：「賜也，賢乎哉？夫我則不暇！」（14.30）\n",
      "31.子曰：「不患人之___不己知___，患其不能也\n",
      "\n",
      "_[9]_    孔子對曰：「俎豆之事，則嘗聞之矣；軍旅之事，___未之學___也\n",
      "\n",
      "_[10]_    」（15.14）\n",
      "15.子曰：「躬自厚，而薄責於人，則遠怨矣！」（15.45）\n",
      "16.子曰：「不曰：『如之何，如之何』者，吾末如之何也已矣？」（15.46）\n",
      "17.子曰：「群居終日，言不及義，好行小慧；難矣哉！」（15.17）\n",
      "18.子曰：「君子義以為質，禮以行之，孫以出之，信以成之；君子哉！」（15.18）\n",
      "19.子曰：「君子病無能焉，不病人之___不己知___也\n",
      "\n",
      "_[11]_    」「日月逝矣！歲___不我與___！」孔子曰：「諾，吾將仕矣！」（17.1）\n",
      "2.  子曰：「性相近也，習相遠也\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Example sentences for ADV-PRON-VERB in the Analects\\n\")\n",
    "highlighted_sentences = find_and_highlight_sentences(analects, pattern_str2_analects, start_chars=('不','未','無'))\n",
    "for i in range(len(highlighted_sentences)):\n",
    "    print(f'_[{i+1}]_    {highlighted_sentences[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f6121648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences for PRON-PRON-VERB\n",
      "\n",
      "_[1]_    岂曰能贤？光昭先君之令德，可不务乎？吾___子其无___废先君之功\n",
      "\n",
      "_[2]_    ”曰 ：“___子其行___乎 ！”大子曰\n",
      "：“君实不察其罪，被此名也以出，人谁纳我？”\n",
      "    十二月戊申，缢于新城\n",
      "\n",
      "_[3]_    ”\n",
      "公谓公孙枝曰 ：“夷___吾其定___乎？对曰 ：“臣闻之，唯则定国\n",
      "\n",
      "_[4]_    子鱼曰 ：“祸其在此乎！君欲已甚，___其何以___堪之？”于是楚执宋公以伐宋\n",
      "\n",
      "_[5]_    叔詹曰 ：“楚王其不没乎！为礼卒于无别，无别不可谓礼，___将何以___没？”诸侯是以知其不遂霸也\n",
      "\n",
      "_[6]_    其波及 晋国者，君之余也，___其何以___报君？”曰 ：“虽然，何以报我？ “对曰 ：“若以君之灵，得反晋国，晋、楚治兵，遇于中原，其辟君三舍\n",
      "\n",
      "_[7]_    ___吾是以___失楚，又何祀焉？”秋，楚成得臣、斗宜申帅师灭夔，以夔子归\n",
      "\n",
      "_[8]_    救而弃之，谓诸侯何？楚有三施，我有三怨，怨仇已多，___将何以___战？不如私许复曹、卫以携之，执宛春以怒楚，既战而后图之\n",
      "\n",
      "_[9]_    ___吾何以___堪之？”\n",
      "东门襄仲将聘于周，遂初聘于晋\n",
      "\n",
      "_[10]_    秦以胜归，___我何以___报？”乃皆出战，交绥\n",
      "\n",
      "_[11]_    今天或\n",
      "\n",
      "者大警晋也，而又杀林父以重楚胜，其无乃久不竞乎？林父之事君也，进思尽忠，退思补过，社稷之卫也，若___之何杀___之？夫其败也，如日月之食焉，何损于明？”晋侯使复其位\n",
      "\n",
      "_[12]_    背盟，不祥；欺大国，不义；神人弗助，___将何以___胜？”不听，遂伐茅戎\n",
      "\n",
      "_[13]_    此车一人殿之，可以集事，若之___何其以___病败君之大 事也？擐甲执兵，固即死也\n",
      "\n",
      "_[14]_    ’其___此之谓___乎！有上不吊，其谁不受乱 ？吾亡无日矣 ！”君子曰 ：“如惧如是，斯不亡矣\n",
      "\n",
      "_[15]_    ‘七年之中，一与一夺，二三孰甚焉！士之二三，犹丧妃耦，而况霸主？霸主将德是以 ，而二三之 ，___其何以___长有诸侯乎？\n",
      "《诗》曰：‘犹之未远 ，是用大简\n",
      "\n",
      "_[16]_    与渠丘公立于池上，曰\n",
      "：“城已恶 ！”莒子曰 ：“辟陋在夷，___其孰以___我为虞？”对曰\n",
      " ：“夫狡焉思启封疆以利社稷者，何国蔑有？唯然，故多大国矣，唯或思或纵也\n",
      "\n",
      "_[17]_    妇人怒曰 ：“己不能庇其伉俪而亡之，又不能字人之孤而杀之，___将何以___终？”遂誓施氏\n",
      "\n",
      "_[18]_    过申，子反入见申叔时，曰 ：“师___其何如___？”对曰 ：“德、刑、详、义、礼、信，战之器也\n",
      "\n",
      "_[19]_    志失列丧，___将何以___战？楚惧不可用也\n",
      "\n",
      "_[20]_    五会之信，今将背之，虽楚救我，将安用之？亲我无成，鄙___我是欲___，不可従也\n",
      "\n",
      "_[21]_    ’其___是之谓___乎？周之兴也，其《诗》曰：‘仪刑文王，万邦作孚\n",
      "\n",
      "_[22]_    ”季孙曰 ：“我有四封，而诘其盗，何故不可？子为司寇，将盗是务去，若之何不能？”武仲曰： “子召外盗而大礼焉，何以止吾盗？子为正卿，而来外盗；使纥去之，___将何以___能？庶其窃邑于邾以来，子以姬氏妻之，而与之邑，其従者皆有赐焉\n",
      "\n",
      "_[23]_    若之___何其以___虎也弃社稷？子为善，谁敢不勉？多杀何为\n",
      "？”宣子说，与之乘，以言诸公而免之\n",
      "\n",
      "_[24]_    宣子曰： “昔匄之祖，自虞以上，为陶唐氏 ，在夏为御龙氏 ，在商为豕韦氏，在周为唐杜氏，晋主夏盟为范氏，其___是之谓___乎？”穆叔曰 ：“以豹所闻，___此之谓___世禄，非不朽也\n",
      "\n",
      "_[25]_    ’今宁子视君不如弈棋 ，___其何以___免乎\n",
      "？弈者举棋不定，不胜其耦\n",
      "\n",
      "_[26]_    ’乐喜之谓乎 ？‘何以恤我，___我其收___之\n",
      "\n",
      "_[27]_    吾闻卫康叔 、武公之德如是，是其《卫风》乎 ？”___为之歌___《王》，曰 ：“美哉\n",
      "！思而不惧，其周之东乎？”___为之歌___《郑》，曰 ：“美哉 ！其细已甚，民弗堪也 ，是其先亡乎 ！”___为之歌___《齐》，曰 ：“美哉！泱泱乎！大风也哉！表东海者，其大公乎！国未可量也\n",
      "\n",
      "_[28]_    “___为之歌___《豳》，曰 ：“美哉！荡乎 ！乐而不淫，其周公之东乎？”___为之歌___《秦》，曰 ：“___此之谓___夏声\n",
      "\n",
      "_[29]_    夫能夏则大，大之至也，其周之旧乎？”___为之歌___《魏》，曰 ：“美哉 ！沨沨乎！大而婉，险而易行，以德辅此 ，则明主也\n",
      "\n",
      "_[30]_    ”___为之歌___《唐》，曰 ：“思深哉！其有陶唐氏之遗民乎？不然，何忧之远也？非令德之后，谁能若是？”___为之歌___《陈》，曰 ：“国无主 ，其能久乎？”自《郐》以下无讥焉\n",
      "\n",
      "_[31]_    ___为之歌___《小雅》，曰：“美哉！思而不贰 ，怨而不言 ，其周德之衰乎？犹有先王之遗民焉\n",
      "\n",
      "_[32]_    “___为之歌___《大雅》，曰 ：“广哉！熙熙乎！曲而有直体 ，其文王之德乎？”___为之歌___《颂》，曰 ：“至矣哉 ！直而不倨，曲而\n",
      "不屈，迩而不逼，远而不携，迁而不淫，复而不厌，哀而不愁，\n",
      "乐而不荒，用而不匮，广而不宣，施而不费，取而不贪，处而不底，行而不流，五声和，八风平，节有度，守有序，盛德之所同也\n",
      "\n",
      "_[33]_    今吾子坏之，虽従者能戒，其若异客何？以敝邑之为盟主，缮完葺墙，以待宾客，若皆毁之，___其何以___共命？寡君使匄请命\n",
      "\n",
      "_[34]_    敢请执事，___将何以___命之？虽君之有鲁丧，亦敝邑之忧也\n",
      "\n",
      "_[35]_    ’终之实难，令尹其将不免 ？”公曰： “___子何以___知之？”对曰 ：“《诗》云：‘敬慎威仪，惟民之则\n",
      "\n",
      "_[36]_    ”子羽曰 ：“当璧犹在，假而不反，___子其无___忧乎 ？”齐国子曰 ：“吾代二子愍矣 ！”陈公子招曰 ：“ 不忧何成，二子乐矣\n",
      "\n",
      "_[37]_    ’三大夫兆忧 ，能无至乎？言以知物，其___是之谓___矣\n",
      "\n",
      "_[38]_    民弗堪也，___将何以___终？夫以强取，不义而克，必以为道\n",
      "\n",
      "_[39]_    司马侯问焉，曰 ：“子之车，尽于此而已乎？”对曰 ：“___此之谓___多矣！若能少此，___吾何以___得见？”女叔齐以告公，且曰 ：“秦公子必归\n",
      "\n",
      "_[40]_    叔向曰 ：“齐 ___其何如___？”晏子曰 ：“此季世也，吾弗知\n",
      "\n",
      "_[41]_    君子曰： “礼，其人之急也乎！伯石之汰也，一为礼于晋，犹荷其禄，况以礼终始乎？《诗》曰：‘人而无礼 ，胡不遄死？’其___是之谓___乎 ！”\n",
      "    初，州县，栾豹之邑也\n",
      "\n",
      "_[42]_    且吴社稷是卜，岂为一人？使臣获衅军鼓，而敝邑知备，以御不虞，其为吉孰大焉？国之守龟，___其何事___不卜？一臧一否，其谁能常之？城濮之兆，其报在邲\n",
      "\n",
      "_[43]_    ’其___是之谓___乎 ？是宫也成，诸侯必叛，君必有咎，夫子知之矣\n",
      "\n",
      "_[44]_    宴语之不怀，宠光之不宣，令德之不知，同福之不受，___将何以___在？”\n",
      "    齐侯、卫侯、郑伯如晋，朝嗣君也\n",
      "\n",
      "_[45]_    其贵亡矣，其宠弃矣，民无怀焉，国无与焉，___将何以___立？”宣子曰 ：“齐桓、晋文，不亦是乎？”对曰 ：“齐桓，卫姬之子也，有宠于僖\n",
      "\n",
      "_[46]_    献无异亲，民无异望，天方相晋，___将何以___代文？此二君者，异于子干\n",
      "\n",
      "_[47]_    王灵不及，拜戎不暇，___其何以___献器？”王曰 ：“叔氏，而忘诸乎？叔父唐叔，成王之母弟也，其反无分乎？密须之鼓，与其大路，文所以大蒐也\n",
      "\n",
      "_[48]_    夫大国之人，令于小国，而皆获其求，___将何以___给之？一共一否，为罪滋大\n",
      "\n",
      "_[49]_    诸侯其有火灾乎？”梓慎曰 ：“往年吾见之，___是其征___也，火出而见\n",
      "\n",
      "_[50]_    凡有季氏与无，于___我孰利___？”皆曰： “无季氏，是无叔孙氏也\n",
      "\n",
      "_[51]_    ___女何以___为哉？夫有尤物，足以移人，苟非德义，则必有祸\n",
      "\n",
      "_[52]_    今弃是度也，而为刑鼎，民在鼎矣，何以尊贵？贵何业之守？贵贱无序，何以为国？且夫宣子之刑，夷之蒐也，晋国之乱制也，若___之何以___为法？蔡史墨曰 ：“范氏、中行氏其亡乎！中行寅为下卿，而干上令，擅作刑器，以为国法，是法奸也\n",
      "\n",
      "_[53]_    ’若之___何其使___蔡先卫也？武王之母弟八人，周公为大宰，康叔为司寇，聃季为司空，五叔无官，岂尚年哉！曹，文之昭也；晋，武之穆也\n",
      "\n",
      "_[54]_    鞅何知焉？”献子谓简子曰 ：“鲁人患阳虎矣，孟孙知其衅，以为必适晋，故强___为之请___，以取入焉\n",
      "\n",
      "_[55]_    民保于城,城保于德,失二德者，危，将焉保？”孟孙曰： “ 二三子以___为何如___ ？恶贤而逆之？”对曰 ：“禹合诸侯于涂山，执玉帛者万国\n",
      "\n",
      "_[56]_    若夏盟于鄫衍，秋而背之，成求而不违，四方诸侯，___其何以___事君？且鲁赋八百乘，君之贰也\n",
      "\n",
      "_[57]_    ”___子我曰___ ：“何害？___是其在___我也\n",
      "\n",
      "_[58]_    ___其何以___为诸侯主 ？先民有言曰 ：‘无秽虐士\n",
      "\n",
      "_[59]_    弑王，不祥，焚库，无聚，___将何以___守矣？”乞曰 ：“有楚国而治其民，以敬事神，可以得祥，且有聚矣，何患？”弗従\n",
      "\n",
      "_[60]_    “叶公曰 ：“王子而相国，过___将何为___？”他日，改卜子国而使 为令尹\n",
      "\n",
      "_[61]_    ”\n",
      "    卫出公自城锄使以弓问子赣，且曰 ：“___吾其入___乎？”子赣 稽首受弓，对曰 ：“臣不识也\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highlighted_sentences = find_and_highlight_sentences(zuozhuan, pattern_str1_zuozhuan, is_question = True)\n",
    "print(\"Example sentences for PRON-PRON-VERB\\n\")\n",
    "for i in range(len(highlighted_sentences)):\n",
    "    print(f'_[{i+1}]_    {highlighted_sentences[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "759af832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences for ADV-PRON-VERB\n",
      "\n",
      "_[1]_    于天子，则诸卿皆行，公___不自送___\n",
      "\n",
      "_[2]_    人无衅焉，妖___不自作___\n",
      "\n",
      "_[3]_    纳而不定 ，废而不立，以德为怨，秦___不其然___\n",
      "\n",
      "_[4]_    初，楚子玉自为琼弁玉缨，___未之服___也\n",
      "\n",
      "_[5]_    先轸曰 ：“匹夫逞志于君而无讨 ，敢___不自讨___乎 ？”免胄入狄 师，死焉\n",
      "\n",
      "_[6]_    谓上___不我知___，黜而宜，乃知我矣\n",
      "\n",
      "_[7]_    以讨召诸侯，而以贪归之，无乃不可乎？王曰\n",
      " ：“善哉 ！”吾___未之闻___也\n",
      "\n",
      "_[8]_    若___不我纳___，今将驰矣\n",
      "\n",
      "_[9]_    ’今楚师至，晋___不我救___，则楚强矣\n",
      "\n",
      "_[10]_    楚弱 于晋，晋___不吾疾___也\n",
      "\n",
      "_[11]_    荀偃令曰 ：“鸡鸣而驾，塞井夷灶，唯余马首是瞻！ “栾黡曰 ：“晋国之命，___未是有___也\n",
      "\n",
      "_[12]_    受楚之功而取货于郑，不可谓国，秦___不其然___\n",
      "\n",
      "_[13]_    子不辟宗，何也？”曰 ：“宗___不余辟___，余独焉辟 之？赋诗断章，余取所求焉，恶识宗？”癸言王何而反之，二人皆嬖，使执寝戈，而先后之\n",
      "\n",
      "_[14]_    子皮止之，众曰 ：“人___不我顺___，何止焉？”子皮曰 ：“夫人礼于死者，况生者乎？”遂自止之\n",
      "\n",
      "_[15]_    公薨之月，子产相郑伯以如晋，晋侯以我丧故，___未之见___也\n",
      "\n",
      "_[16]_    子产曰：“少，未知可否?”子皮曰： “愿，吾爱之，___不吾叛___也\n",
      "\n",
      "_[17]_    ”叔向曰 ：“善哉！肸___未之闻___也\n",
      "\n",
      "_[18]_    晋为盟主，其或者___未之祀___也乎？”韩子祀夏郊，晋侯有间，赐子产莒之二方鼎\n",
      "\n",
      "_[19]_    取郠之役，莒人诉于晋，晋有平公之丧，___未之治___也，故辞公\n",
      "\n",
      "_[20]_    今郑人贪赖其田，而___不我与___\n",
      "\n",
      "_[21]_    降服而对，曰 ：“臣过失命，___未之致___也\n",
      "\n",
      "_[22]_    ”不吉，投龟，诟天而 呼曰 ：“是区区者而___不余畀___，余必自取之\n",
      "\n",
      "_[23]_    国人请为□焉，子产弗许，曰 ：“我斗，龙___不我觌___也\n",
      "\n",
      "_[24]_    ”对曰 ：“吾 由子事公孟，子假吾名焉，故___不吾远___也\n",
      "\n",
      "_[25]_    无民而能逞其志者，___未之有___也\n",
      "\n",
      "_[26]_    会曰 ：“偻句___不余欺___也\n",
      "\n",
      "_[27]_    事若克，季子虽至，___不吾废___也\n",
      "\n",
      "_[28]_    骄而不亡者，___未之有___也\n",
      "\n",
      "_[29]_    越不为沼，吴其泯矣，使医除疾，而曰：‘必遗类焉’者，___未之有___也\n",
      "\n",
      "_[30]_    甲兵之事，___未之闻___也\n",
      "\n",
      "_[31]_    何世有职焉，自襄以来，___未之改___也\n",
      "\n",
      "_[32]_    以国之多难，___未女恤___也\n",
      "\n"
     ]
    }
   ],
   "source": [
    "highlighted_sentences = find_and_highlight_sentences(zuozhuan, pattern_str2_zuozhuan, start_chars=('不','未','无'))\n",
    "print(\"Example sentences for ADV-PRON-VERB\\n\")\n",
    "for i in range(len(highlighted_sentences)):\n",
    "    print(f'_[{i+1}]_    {highlighted_sentences[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12462089-fec0-40fc-9ee8-490ae01209e6",
   "metadata": {},
   "source": [
    "## Findings\n",
    "#### SOV in Interrogative Sentences\n",
    "Looking at the example sentences, I found a few interesting cases that exhibit similar shifts from subject-verb-object to subject-object-verb in the context of an interrogative sentence. These examples were found by filtering out all instances where there was no question mark, which limits our search to cases that are more likely to exhibit grammatical case.\n",
    "* Analects 其何以, 吾何以, 斯之謂，爾何如\n",
    "* Zuozhuan 吾其定, 其何以, 吾何以, 其何如, [某某某]将何以, 吾其入\n",
    "\n",
    "After looking at the other cases in the Analects, it's most likely the case that 如之何 as a set phrase is throwing off the results, as there is no punctuation after 何, so the primitive search method using POS gives undesirable results, such as:\n",
    "* 何其聞, 何其徹, 何其廢, 何其拒\n",
    "\n",
    "This likely indicates that there is a better method for finding SOV cases.\n",
    "\n",
    "#### SOV in Negation Sentences\n",
    "There were also many more cases of the negative-pronoun-verb pattern. These instances were found by modifying my _find_and_highlight_sentences_ function to only look up the patterns where the first character was either 不 or 未 for simplicity and to narrow down the search to exclude patterns that are highly unlikely to be in a negation sentence.\n",
    "* Analects: 未之見, 未之有, 未之學, 不吾知, 不吾以, 莫予違, 不己知, 不我與\n",
    "* Zuozhuan: 不我知, 不我纳, 不我救, 不吾疾, 不吾叛,  不我与, 不余畀, 不我觌, 不吾远, 不余欺, 不吾废\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952726a1-59d4-4435-a8bf-a951cbc869b3",
   "metadata": {},
   "source": [
    "## Future Research and Direction\n",
    "I did not look for other cases of grammatical case within the Analects or Zuozhuan to see how they were tagged. Next, I could also compare the frequency of certain grammatical patterns across texts from different time periods to highlight certain trends. For example, when do we start to see SOV in interrogative and negation sentences turn into SVO and how quickly does that transition occur? More importantly, I did not consider other methods for identifying the patterns outside of POS tags. It is possible other methods such as supervised machine learning could be utilized to identify patterns without the rigid framework of some pattern of POS tags. Something along the lines of neural networks could provide weights for features based on context that could perhaps be difficult for a human to see. \n",
    "\n",
    "It is entirely possible to turn this entire project into a more intuitive and web-based tool for others interesting in research of Chinese grammar to use. In that case, the users could either (A) upload their documents or (B) choose to work from a list of works that have already been processed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
